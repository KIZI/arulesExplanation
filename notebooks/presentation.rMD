---
title: "xML Challenge"
author: Tomas Kliegr, Jiri Filip, University of Economics, Prague, tomas.kliegr@vse.cz
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Table of Contents
1. [Methods used in a nutshell](#methods)
2. [Submission no. 1: QCBA](#subm1)
    1. [Data format for instance explanations](#data_format_instance)
    2. [Data format for class explanations](#data_format_class)
    3. [Model accuracy vs model interpretability](#accsize)
3. [Submission no. 2: CBA ](#cba)
4. [Interpretability of QCBA and CBA models](#alternative)    
    1. [General properties](#prop)
    2. [Psychological aspects](#psy)
    3. [Monotonicity](#mon)
    4. [QCBA comparison with related intepretable models on UCI datasets](qcbaUCI)
5. [CBA/QCBA Ecosystem](#ecosystem)
    1. [EasyMiner-Interactive web interface](#interactive)
    2. [Other implementations (R, Python, Java, C)](#impl)

Several sections contain embedded video demonstrations.

# Methods used in a nutshell <a name="methods"></a>

This submission is based on *Monotonicity-exploiting Association Rule Classification Algorithm* (MARC), as implemented  in our  [qCBA R](https://github.com/kliegr/QCBA) package:

    Kliegr, Tomas. "Quantitative CBA: Small and Comprehensible Association Rule Classification Models." arXiv preprint arXiv:1711.10166 (2017).

QCBA package postprocesses models created by  association rule classification algorithm CBA, making them more comprehensible. The reference for CBA is:

    Ma, Bing Liu Wynne Hsu Yiming, and Bing Liu. "Integrating classification and association rule mining." Proceedings of the fourth international conference on knowledge discovery and data mining. 1998.
    
**How QCBA  works?** QCBA uses original, undiscretized numerical attributes to optimize the discovered association rules, refining the boundaries of literals in the antecedent of the rules produced by CBA. Some rules as well as literals from the rules can consequently be removed, which makes the resulting classifier smaller. One-rule classification and crisp rules make CBA classification models possibly most comprehensible among all association rule classification algorithms. These viable properties are retained by QCBA. The postprocessing is conceptually fast, because it is performed on a relatively small number of rules that passed data coverage pruning in CBA. Benchmark of our QCBA approach on 22 UCI datasets shows average 53% decrease in the total size of the model as measured by the total number of conditions in all rules. Model accuracy remains on the same level as for CBA.

**A visual tutorial for QCBA** is at https://nb.vse.cz/~klit01/qcba/tutorial.html

More details on monotonicity support in the algorithm is provided in the [Monotonicity section](#mon).

# Submission no. 1 - QCBA (primary solution)  <a name="subm1"></a>
The baseline submission uses a default run of the QCBA algorithm as implemented in the qCBA R package.

The solution consists of the following files:

* `solution/qcba-instance-explanations.csv` -- explanations for all instances (short version without justifications),
* `solution/qcba-instance-explanations-with-justifications.csv` -- explanations with justifications (full version),
* `solution/qcba-class-explanations-good.csv` -- the list of all rules describing class "good",
* `solution/qcba-class-explanations-bad.csv` -- the list of all rules describing class "bad".

The code for reproduction of the results is in the script `scripts/explainQCBA.R`.

Note that to generate the explanations for the xML challenge, we had to extend the `qCBA` package.
Since updating the package in the central R repository CRAN takes some time,  the reproduction code relies on qCBA code on GitHub.
QCBA can be installed from  CRAN with:

`library(devtools)`

`install_github("kliegr/qCBA")`

We also included the source code of the `qCBA` and `arc` packages into the `source_code` directory.

Also note that the default configuration for CBA  uses a heuristic algorithm to set values of the CBA hyperparameters. We opted to use the default setup to avoid risk of overfitting to the xML challenge dataset.
The down side is that the  heuristic algorithm is controlled by available time, which means that on each execution a slightly different results might be produced.


### Data format for INSTANCE explanations <a name="data_format_instance"></a>

The submission contains two files:
* `solution/qcba-instance-explanations.csv` -- explanations for all instances (short version without justifications),
* `solution/qcba-instance-explanations-with-justifications.csv` -- explanations with justifications (full version),

This data format also applies to the second solution.

| instance_index   |      explanation      |  justification | strongest rule supporting alternative class   | predicted class
|----------|:-----------------:|-----------------------:|:------------------:|----------:|
| 1 |  IF PercentTradesWBalance is between 2 (excl) to 44.5 THEN RiskPerformance is Good | ... | ... |  Bad
| 2 | .. | .. |.. | .. |
| 3 | .. | .. |.. | .. |


Meaning of the individual columns:

* **instance_index**: row number from the Heloc dataset
* **explanation**: rule used to classify the instance
* **justification**: backing for the rule used to classify the instance
* **strongest rule supporting alternative class**: not all classifications are clear cut. This column provides the strongest rule in the rule base that supports classification to the alternative class
* **predicted class**: assigned class

Additional information on individual columns follows.

#### EXPLANATION

Example:

    IF ExternalRiskEstimate is between 36 to 70 THEN RiskPerformance is Bad
    
Note that all explanations submitted are rules consisting of maximum condition. Our system is capable of generating more complex rules - with more conditions. As a result of tuning the **target_rule_count** parameter reported <a href="#accsize"> below</a>, we found the optimal value of **target_rule_count** to be relatively small (50). Since the CBA algorithm gives preference to more general rules with higher number of backing instances, this restriction resulted in all generated rules being very simple.

####  JUSTIFICATION

Example:

    There were 2175 clients which match the conditions of this rule in the training dataset. Out of these 1784 are predicted correctly as having RiskPerformance=Bad by this rule. The confidence of the rule is thus 82 %.

The justification provides backing for the rule used to classify the instance in terms of natural language representation of the confidence and support of the rule. The wording of the justifications in our solution is based on frequency formats, which have been shown in psychology to be better understandable than percentages, for references cf. <a href="#psy">below</a>.

#### SUPPORTING THE ALTERNATIVE CLASS
Example:

     The strongest applicable conflicting rule predicting the alternative class Good is {ExternalRiskEstimate=[82;93]} => {RiskPerformance=Good}  This conflicting rule has lower strength of evidence with confidence 0.93 % lower than the selected rule. (81.99 % vs 81.06 %). The weight of evidence of the conflicting rule is 1.14 % smaller compared to the selected rule (1784 cases vs 1665 cases).  Overall, the evidence behind the strongest conflicting rule has both lower strength and weight.

The motivation for this additional field is giving the person in charge of the case the information on how "clear cut" the decision on the  credit application is. To do that, the strongest rule that would classify the instance to the alternative class was looked up and included in this column, along with explanation how much in confidence and support this alternative rule looses on the selected rule that was actually used to classify the instances. Subject to regulatory requirements, the decision maker can refer to the alternative rule when making the final judgment.

As for wording, we decided to adopt the more common terminology used in cognitive science as well as legal domains, using terms  *weight of evidence* and *strength of evidence*, rather than *support* and *confidence*, which are used in the domain of machine learning. These metrics are defined in the following.

### Data format for CLASS explanations <a name="data_format_class"></a>
This data format also applies to the second solution.

| Priority   |   Explanation (RiskPerformance=Bad)      |  Confidence | Support (rel) | Support (abs) | Lift |
|----------|:------------------------------------------------:|:------------:|:-----------:|:-----------:|:-----------:|
| 1 |  If NetFractionRevolvingBurden is between -4.5 and 3.5 (incl.) AND PercentTradesWBalance is between  60.5 and 73.5 |  1| 0.002 | 200 | 1.916|
| 3 | .... | ... | ... | ... | ... |
| 4 | .... | ... | ... | ... | ... |

Note 1: The rules in the model have associated *priority* in the first column. Some priority numbers may be skipped (note the gap between Priority 1 and 3, this is because the rule with priority 2 predicts  Good class and is thus included in the other file.

Note 2: When rules were translated to natural language, the word *between* was considered as inclusive. To denote exclusive boundary, *(excl.)* is appended.  Statement *between -4.5 and 3.5 (excl.)* thus refers to the interval `[-4.5; 3.5)`. 

Note 3. The class explanation files do not contain the default rule.

The meaning of the metrics in columns 3 to 5 is as follows:

* Support (abs): absolute support is defined as `a` (absolute support), where `a` is the number objects (instances, rows) that match the conditions of the rule's antecedent as well as the rule's consequent
* Support (rel): is defined as `a/N`, where `N` is the number of all objects (relative support)
* Confidence: The confidence of a rule is defined as `a/(a+b)`, where `a` is defined as above, and `b` is the number of objects that match the antecedent but not the consequent of the rule
* Lift: See https://en.wikipedia.org/wiki/Lift_(data_mining) 

### Model accuracy vs model interpretability (size)  <a name="accsize"></a>
For classifier building, CBA does not have any tunable parameters. However, the input of CBA is the result of association rule learning,  which can be influenced by user setting of the minimum confidence threshold, min. support threshold and the maximum length of the antecedent. 
Tuning of these parameters can be sometimes difficult, as incorrect setting can result in combinatorial explosion of rules.
As an alternative, our implementation offers a heuristic algorithm, that accepts as a parameter the target number of rules that should be on the input of CBA (`target_rule_count`). In general, the more rules to select from, the bettter accuracy of the resulting model.

In the following plots, we provide an overview of the effect of varying the `target_rule_count` parameter on xML data. The results are based on a single split (90% for training, 10% for testing).

![](figures/sensitivity-acc.png)

![](figures/sensitivity-rules.png)


The results show that a near-optimal setting of the `target_rule_count` parameter for xML data is 50. With this setting, the CBA model composes of 34 rules (some were removed by pruning), and the QCBA model composes of 24 rules. QCBA consistently produces smaller models than CBA. The accuracy of the QCBA model is at`target_rule_count=50` 1% higher than the accuracy of the CBA model. The disadvantage of QCBA compared to CBA is a higher volatility of model accuracy. This setting of `target_rule_count=50` was used for our solution.


The scripts to reproduce the sensitivity analysis is `scripts/sensitivity.R` and `scripts/plot_sensitivity.R`.


# Solution no. 2 - CBA <a name="cba"></a>
The alternative submission uses a default run of CBA algorithm as implemented in our ARC package.
The code for reproduction of the results is in the script `scripts/explainCBA.R`. While QCBA generally  produces smaller models than CBA with the same accuracy and interpretability,  for some applications using a more widely adopted CBA model might be preferrable. For this reason, we also submit a CBA-based solution.

* `solution/cba-instance-explanations.csv` -- explanations for all instances (no justifications). 
* `solution/cba-instance-explanations-with-justifications.csv` -- explanations with justifications
* `solution/cba-class-explanations-good.csv` -- the list of all rules describing class "good".
* `solution/cba-class-explanations-bad.csv` -- the list of all rules describing class "bad".


### QCBA comparison with related intepretable models (UCI datasets) <a name="qcbaUCI"></a>

**CBA vs other association rule (or closely related) classifiers**

The following table presents tomparison between CBA and other association rule (or closely related) classifiers on 26 datasets. *single* refers to single rule classification, *crisp* to whether the rules comprising the classifier are crisp (as opposed to fuzzy), *det.* to whether the algorithm is deterministic with no random element such as genetic optimization, *assoc* corresponds to whether the method is based on association rules. The last three columns (*acc*, *rules* and *time* -- average accuracy, average rule count and average time) are sourced from:

    Alcala-Fdez, Jesús, Rafael Alcala, and Francisco Herrera. "A fuzzy association rule-based classification model for high-dimensional problems with genetic rule selection and lateral tuning." IEEE Transactions on Fuzzy Systems 19.5 (2011): 857-872.




![CBA interpretability, accuracy and speed vs related classifiers, * indicates that the algorithm did not process all datasets](figures/table-comp.png)

The results show that CBA provides second  best result in terms of accuracy (after CPAR), but is much faster than most other implementations. Additionally, it provides 6x less rules than CPAR.

**QCBA vs CBA**

In the following paper, we performed a detailed comparison of the performance of CBA and QCBA on 22 UCI datasets previously used in this domain. 

    Kliegr, Tomas. "Quantitative CBA: Small and Comprehensible Association Rule Classification Models." arXiv preprint arXiv:1711.10166 (2017).

The results show that shows that QCBA results in average 53\% decrease in the total size of the model as measured by the total number of conditions in all rules. Model accuracy remains on the same level as for CBA.

![CBA vs QCBA: accuracy, number of rules, number of conditions (UCI datasets)](figures/table-comp2.png)

More details can be found a https://arxiv.org/pdf/1711.10166.pdf.

# Interpretability of CBA/qCBA   <a name="int"></a>
CBA/qCBA produce a list of crisp rules based on postprocessing the output of class association rule learning. 

### General Properties  <a name="prop"></a>
From the interpretability perspective, CBA/qCBA approach has the following favourable properties:

* Discovered rules are interoperable with Business Rule Systems (BRMS) standards like SBVR or DRL, or prospectively DMN
* The algorithm produces reasonably small number of rules due to built-in rule pruning
* There is a clear rule conflict resolution strategy: the first applicable rule in the list fires and classifies the instance
* User can control the quality of individual rules: it is possible to set thresholds on minimum confidence and support of rules present in the classifier (except for the default rule)

The properties described above are discussed in greater detail in:
     
     Kliegr, T., Kuchař, J., Sottara, D., & Vojíř, S. (2014, August). Learning business rules with association rule classifiers. In  Proceedings of International Web Rule Symposium (RuleML 2014) (pp. 236-250). Springer, Cham.
     
     Vojir, S., Kliegr, T., Hazucha, A., Škrabal, R., & Šimunek, M. (2013). Transforming association rules to business rules: EasyMiner meets Drools. RuleML-2013 Challenge. CEUR-WS. org, 49.

The QCBA algorithm preserves the favourable interpretability properties of CBA.

###  Cognitive grounding for wording of generated explanations  <a name="psy"></a>


The wording of the justifications in our solution is based on frequency formats, which have been shown in psychology to be better understandable than percentages, cf. e.g.:

    Gigerenzer, G., & Hoffrage, U. (1995). How to improve Bayesian reasoning without instruction: frequency formats. Psychological review, 102(4), 684.

A more detailed justification for applying this research to rule-based models is provided in:

    Kliegr, T., Bahník, Š., & Fürnkranz, J. (2018). A review of possible effects of cognitive biases on interpretation of rule-based machine learning models. arXiv preprint arXiv:1804.02969.


### Monotonicity  <a name="mon"></a>
The original CBA algorithm works on nominal (prediscretized) data and cannot take advantage of monotonicity.
The QCBA algorithm was developed under the working name "Monotonicity-exploiting association rule classification" (MARC) in:

    Kliegr, T. (2017). EFFECT OF COGNITIVE BIASES ON HUMAN UNDERSTANDING OF RULE-BASED MACHINE LEARNING MODELS (Doctoral dissertation, Queen Mary University of London).


In many classification and regression problems it is often the case that a domain of a predictor attribute has  a -- ceteris paribus -- monotone relationship with the target class label. 
An example of such monotonicity constraint is that within a certain range, subjective comfort level will be increasing with room temperature, all other variables being equal. 
Our goal was  to incorporate respect for monotonicity into  a classification algorithm.
As a basis algorithm  for our framework we decided to take association rule learning, which is well known for its scalability. For example, it has been reported to be successfully used to mine massive ($p \approx 10^4, N \approx 10^8$) commercial databases (Elements of Statistical Learning, Hastie et al., 2013).

In MARC (QCBA) we do not aim to impose monotonicity as a hard or even soft constraint, which would result in multi-objective optimization: a drop in standard rule quality metrics such as confidence will be accepted as long as monotonicity is ensured or at least improved. 
Instead we readjust association rule output to reflect monotonicity without adversely affecting these metrics. How could this ''win-win`` optimization be possible? Association rule learning and classification operates on prediscretized data, which results in a learned rule often covering a narrower region than it could. We apply the monotonicity constraint when readjusting the rules to better fit the raw data, detaching them from the multidimensional grid, which is the result of the discretization. The intuition behind our approach is briefly exemplified in two dimensions in the following video on a simplified 2D problem describing the effects of humidity and temperature on subjective comfort level.

An example rule discovered on this domain is:

    IF Temperature=(25;30] AND Humidity=(40;60] => Class=4

On the following video, the X axis denotes humidity and Y axis temperature.

<video width="640" height="480" controls>
  <source src="figures/extend.mp4" type="video/mp4">
</video>

The monotonicity assumptions tested  by QCBA are (in the order as they appear in the video):
1. If temperature rises, the Class will still be 4 
2. If temperature drops, the Class will still be 4
3. If humidity drops, the Class will still be 4

**While we did not manage to prepare this for the challenge, it is possible to guide the search only to directions indicated by the expert. **

The extension process is described in greater detail here:  https://nb.vse.cz/~klit01/qcba/tutorial.html


### Editable models <a name="edit"></a>
Following Article 22 of GDPR models should also allow for ``human intervention''.  Unlike most other types of models, QCBA models can be edited or modified by the user. The edit is easy, since individual rules are independent, and the default rule in the end of the classifier ensures that all instances are always covered.

In the following, we demonstrate the  model edit capability built into our EasyMiner software (see below).

<video width="640" height="480" controls>
  <source src="figures/editor.mp4" type="video/mp4">
</video>

# Ecosystem <a name="ecosystem"></a>
### Interactive user interface <a name="interactive"/>
Our group has been working since 2010 on a web-based graphical user interface for association rule learning and later also for classificiation (CBA). EasyMiner allows to visually build CBA models. 

<video width="640" height="480" controls>
  <source src="figures/em-video.mp4" type="video/mp4">
</video>
A **free openly accessible demo** is at http://easyminer.eu (no need to install). 

An early version of the system called SEWEBAR-CMS obtained runner-up prize in RuleML Challenge held at Washington D.C. in 2010 (http://2010.ruleml.org/ruleml-2010-awards.html). 

    Tomáš Kliegr, David Chudán, Andrej Hazucha and Jan Rauch.SEWEBAR-CMS: A System for Postprocessing Data Mining Models

The full first release was published at ECML/PKDD:
    
    Škrabal, R., Šimůnek, M., Vojíř, S., Hazucha, A., Marek, T., Chudán, D., & Kliegr, T. (2012, September). Association rule mining following the web search paradigm. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 808-811). Springer, Berlin, Heidelberg.

The current version of the system is called *EasyMiner* and has recently been published in:

    Vojíř, S., Zeman, V., Kuchař, J., & Kliegr, T. (2018). EasyMiner. eu: Web framework for interpretable machine learning based on rules and frequent itemsets. Knowledge-Based Systems, 150, 111-115.

A **dockerized version** is available at http://www.easyminer.eu/installation. Features of EasyMIner are explained here: http://www.easyminer.eu/feature-tutorial 

Example CBA output for the xML data in extended *PMML* format from the EasyMiner system is in `supplementary_output/easyminer.xml`


### Other implementations (R, Python, Java, C)  <a name="impl"/>

The current submission was produced with our `qCBA` and `arc` R packages, which are available in CRAN -- the official R repository.
There are also several alternative implementations of CBA for other languages.


**R**

* https://cran.r-project.org/web/packages/arc/ (https://github.com/kliegr/arc) (our implementation)
* https://cran.r-project.org/web/packages/qCBA/ (https://github.com/kliegr/QCBA) (our implementation)

**Python**

* https://pypi.org/project/pyARC/ (https://github.com/jirifilip/pyARC) (our implementation)
 
**Java**

* https://github.com/jaroslav-kuchar/rCBA (CBA implementation is in Java, with an R wrapper) 

**C**

* https://cran.r-project.org/web/packages/arulesCBA/ (CBA implementation is in C, with an R wrapper)



